\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T2A, T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\newcommand{\R}{\mathbb{R}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bA}{\mathbf{A}}


\title{Ускоренные методы нулевого порядка в гладкой выпуклой стохастической оптимизации}

\author{
	Хафизов Фанис \\
	\texttt{khafizov.fa@phystech.edu} \\
	%% examples of more authors
	\And
	Богданов Александр \\
	\texttt{bogdanov.ai@phystech.edu} \\
	\And
	Безносиков Александр \\
	\texttt{beznosikov.an@phystech.edu}
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{Ускоренные методы нулевого порядка}
\renewcommand{\undertitle}{}
%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Ускоренные методы нулевого порядка в гладкой выпуклой стохастической оптимизации},
pdfsubject={Стохастическая оптимизация},
pdfauthor={Хафизов Ф.А., Богданов А.И., Безносиков А.Н.},
pdfkeywords={Методы нулевого порядка},
}

\begin{document}
\maketitle

\begin{abstract}
Данная работа посвящена задачи оптимизации без доступа к градиенту целевой функции $f(x)$, из-за чего нам надо как-то его оценивать. Рассматривается безградиентный метод JAGUAR, использующий информацию о предыдущих вызовах и требует $\mathcal{O}(1)$ оракульных вызовов. Мы применяем эту аппроксимацию в ускоренном методе градиентного спуска и докажем его сходимость для выпуклой задачи оптимизации. Также сравним метод JAGUAR с другими известными способами на экспериментах.

\end{abstract}

\keywords{методы нулевого порядка, стохастическая оптимизация}

\section{Введение}
\subsection{Мотивация}
Стохастические градиентные методы являются необходимыми в решении различных оптимизационных задач. Однако в нынешних проблемах машинного обучения возникает потребность оценивать градиент, ввиду, например, дороговизны его подсчета, либо же незнания явного вида минимизируемой функции. Тогда на помощь приходят методы нулевого порядка. Так как имеется доступ только к значениям целевой функции $f$ в различных точках $x \in \R^d$, то необходимо строить методы, аппроксимирующие градиент, используя конечные суммы значений целевой функции.\\
Одним из возможных усложнений задачи является добавление шума к оракулу: вместо $f(x)$ он будет возвращать $f(x) + \delta (x)$. Более того, виды шума разделяются на стохастический \cite{lucchi2022theoretical} или детерминированный \cite{lobanov2023zeroorder}.\\
Другую важную роль в решении оптимизационных задач играют ускоренные методы. Они как правило имеют более быструю сходимость по сравнению со стандартными алгоритмами. Предложенный Нестеровым \cite{Nesterov1983AMF} быстрый градиентный метод является классическим примером.
\subsection{Смежные работы}
Впервые метод JAGUAR аппроксимации градиента был предложен в \cite{bogdanov2024aspects}, доказана сходимость для методов Франка-Вульфа и градиентного спуска для невыпуклой, выпуклой, и сильно-выпуклой задач. Также есть модификация алгоритма Франка-Вульфа для стохастического случая (рассмотрены случаи one-point feedback и two-point feedback).\\
В работе \cite{beznosikov2023order} рассмотрены ускоренные методы первого порядка в невыпуклых и сильно выпуклых задачах оптимизации, содержащих марковский шум. Там при аппроксимации градиента использовался рандомизированный размер батча. Результаты и подходы оттуда адаптированы под метод JAGUAR в данной работе.\\
State-of-the-art решения в области безградиентной оптимизации собраны в \cite{gasnikov2024randomized}. Представлены рандомизированные методы нулевого порядка, однако лишь для нестохастического случая.\\
Ускоренный безградиентный метод в гладкой выпуклой стохастической оптимизации был предложен в \cite{gorbunov2020accelerated}, но лишь в случае two-point feedback, который далеко не всегда реализуется на практике.
\subsection{Предложения}
Предлагается ускоренный метод, в качестве аппроксимации градиента использующий JAGUAR. Для него получены теоремы о скорости сходимости при разных ограничениях на целевую функцию (выпуклая, сильно-выпуклая, невыпуклая). Теоретическая часть подкреплена численным экспериментом: классификация на датасете небольшого размера и минимизация квадратичной функции. Проведено сравнение с методами $l_2$-smoothing и полной аппроксимацией градиента.


\section{Постановка задачи}
Как уже было упомянуто, задача подразделяется на детерминированный случай, когда $f_\delta$ зависит только от $x$, и стохастический, в котором есть зависимость от случайного вектора $\xi \sim \pi$.
\subsection{Детерминированный случай}
В этом разделе рассматривается детерминированная задача оптимизации:
\begin{equation}
 \min\limits_{x \in \R^d} f(x).
\end{equation}
Мы полагаем, что есть доступ только к оракулу нулевого порядка, то есть, мы можем получать только значения $f(x)$, но не градиента $\nabla f(x)$. Следовательно, нужно как-то аппроксимировать градиент $\nabla f(x)$. Также предполагается, что оракул возвращает
\begin{equation}
 f_\delta (x) = f(x) + \delta (x).
\end{equation}
Для аппроксимации градиента используется следующая разностная схема:
\begin{equation}
\widetilde{\nabla}_i f_\delta (x) := \frac{f_\delta (x + \tau e_i) - f_\delta (x - \tau e_i)}{2\tau} e_i,
\label{diff_scheme}
\end{equation}
где $e_i$ --- вектор из стандартного базиса в $\R^d$, $\tau > 0$ --- достаточно мало.

\subsection{Стохастический случай}
В этом же разделе рассматривается более общая стохастическая задача:
\begin{equation}
 \min\limits_{x \in \R^d} f(x) := \mathsf{E}_{\xi \sim \pi}[f(x, \xi)],
\end{equation}
где $\xi$ --- случайный вектор из неизвестного распределения $\pi$. Здесь мы так же считаем, что у нас нет доступа к градиенту $\nabla f(x, \xi)$, а оракул нулевого порядка возвращает зашумленное значение функции $f_\delta (x, \xi) := f(x, \xi) + \delta (x, \xi)$.\\
Разделяются два вида аппроксимации градиента в стохастическом случае:
\begin{itemize}
 \item Two-point feedback (TPF)
 \begin{equation}
  \widetilde{\nabla}_i f_\delta (x, \xi) := \frac{f_\delta (x + \tau e_i, \xi) - f_\delta (x - \tau e_i, \xi)}{2\tau} e_i,
 \end{equation}
 \item One-point feedback (OPF)
 \begin{equation}
  \widetilde{\nabla}_i f_\delta (x, \xi^+, \xi^-) := \frac{f_\delta (x + \tau e_i, \xi^+) - f_\delta (x - \tau e_i, \xi^-)}{2\tau} e_i,
 \end{equation}
\end{itemize}
где $\xi, \xi^+, \xi^- \sim \pi$, $e_i$ --- базисный вектор в $R^d$, $\tau > 0$ --- достаточно мало.\\
В случае TPF в обоих точках, где мы вызываем оракула, одно и то же значение $\xi$, что может быть тяжело реализуемо. Также TPF является частным случаем OPF ($\xi^+ = \xi^- = \xi$).

\section{Вычислительный эксперимент}
В эксперименте рассматриваются две задачи: минимизация квадратичной функции и логистическая регрессия на датасете mushrooms. Ускоренный градиентный спуск сравнивается со стандартным градиентным спуском и с методом Нестерова. В качестве аппроксимации градиента используется разностная схема (\ref{diff_scheme}), взятая по каждому базисному вектору $e_i, i = \overline{1, d}$.
\subsection{Квадратичная задача}
В качестве целевой функции выступает
\begin{equation}
 f(x) = x^T A x - b^T x + c,
\end{equation}
где $A \in \mathbb{S}_d$ --- случайная симметричная матрица с собственными значениями на отрезке $[\mu, L]$, $b \in \R^d, c \in \R$ --- случайные.\\
Все методы запускаются из одной и той же случайной точки $x^0 \in \R^d$. Также эксперименты разделяются по типу оракула (детерминированный, OPF, TPF).\\
Так как в методах нулевого порядка мы хотим получить как можно лучшее приближение оптимума при минимальном количестве оракульных вызовов, в качестве осей графика взята зависимость $\frac{f(x^k) - f(x^*)}{f(x^0) - f(x^*)}(\textnormal{OracleCalls}(k))$, где $x^k$ --- значение аргумента на $k$-й итерации, $x^*$ --- точка оптимума, найденная численно, $x^0$ --- стартовая точка алгоритма, $\textnormal{OracleCalls}(k)$ --- суммарное количество вызовов оракула за $k$ итераций.
\begin{figure}[!htbp]
\centering
  \includegraphics[width=0.7\textwidth]{../figures/Non_stochastic_quadratic_GD_AGD_Nesterov.pdf}
 \caption{Зависимость относительной ошибки $\frac{f(x^k) - f(x^*)}{f(x^0) - f(x^*)}$ от числа оракульных вызовов для методов градиентного спуска, ускоренного градиентного спуска и Нестерова на квадратичной задаче минимизации}
  \label{fig:non-stochastic_logreg}
\end{figure}

\subsection{Логистическая регрессия}
Оптимизируется модель логистической регрессии с $L_2$-регуляризацией вида
\begin{equation}
 \min\limits{w \in R^d} f(w) = \frac{1}{m}\sum\limits_{k = 1}^m \log(1 + \exp(-y_k \cdot (Xw)_k)) + \lambda \|w\|_2^2,
\end{equation}
где взято $\lambda = 0,05$.\\
В логистической регрессии используется датасет mushrooms из библиотеки LibSVM. В качестве осей графика также применяется $\frac{f(x^k) - f(x^*)}{f(x^0) - f(x^*)}(\textnormal{OracleCalls}(k))$.
\begin{figure}[!htbp]
\centering
  \includegraphics[width=0.7\textwidth]{../figures/Non_stochastic_Logreg_GD_AGD_Nesterov.pdf}
 \caption{Зависимость относительной ошибки $\frac{f(x^k) - f(x^*)}{f(x^0) - f(x^*)}$ от числа оракульных вызовов для методов градиентного спуска, ускоренного градиентного спуска и Нестерова на задаче логистической регрессии}
  \label{fig:non-stochastic_logreg}
\end{figure}
\subsection{Предварительный отчет}
В проведенных экспериментах ускоренный метод показал себя лучше других, хотя, например, в квадратичной задаче поначалу он ведет себя намного хуже. В задаче логистической регрессии рассматриваемый метод показывает линейную скорость сходимости по функции, как и метод градиентного спуска, однако первый оказывается быстрее.
\bibliographystyle{plain}
\bibliography{Khafizov2024AcceleratedZeroOrderMethods}

\end{document}
