\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T2A, T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\newcommand{\R}{\mathbb{R}}

\title{Ускоренные методы нулевого порядка в гладкой выпуклой стохастической оптимизации}

\author{
	Хафизов Фанис \\
	\texttt{khafizov.fa@phystech.edu} \\
	%% examples of more authors
	\And
	Богданов Александр \\
	\texttt{bogdanov.ai@phystech.edu} \\
	\And
	Безносиков Александр \\
	\texttt{beznosikov.an@phystech.edu}
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{}

\renewcommand{\shorttitle}{Ускоренные методы нулевого порядка}
\renewcommand{\undertitle}{}
%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Ускоренные методы нулевого порядка в гладкой выпуклой стохастической оптимизации},
pdfsubject={Стохастическая оптимизация},
pdfauthor={Хафизов Ф.А., Богданов А.И., Безносиков А.Н.},
pdfkeywords={Методы нулевого порядка},
}

\begin{document}
\maketitle

\begin{abstract}
Данная работа посвящена задачи оптимизации без доступа к градиенту целевой функции $f(x)$, из-за чего нам надо как-то его оценивать. Рассматривается безградиентный метод JAGUAR, использующий информацию о предыдущих вызовах и требует $\mathcal{O}(1)$ оракульных вызовов. Мы применяем эту аппроксимацию в ускоренном методе градиентного спуска и докажем его сходимость для выпуклой задачи оптимизации. Также сравним метод JAGUAR с другими известными способами на экспериментах.

\end{abstract}

\keywords{методы нулевого порядка, стохастическая оптимизация}

\section{Введение}
\subsection{Мотивация}
Стохастические градиентные методы являются необходимыми в решении различных оптимизационных задач. Однако в нынешних проблемах машинного обучения возникает потребность оценивать градиент, ввиду, например, дороговизны его подсчета, либо же незнания явного вида минимизируемой функции. Тогда на помощь приходят методы нулевого порядка. Так как имеется доступ только к значениям целевой функции $f$ в различных точках $x \in \R^d$, то необходимо строить методы, аппроксимирующие градиент, используя конечные суммы значений целевой функции.\\
Одним из возможных усложнений задачи может быть добавлением шума к оракулу: вместо $f(x)$ он будет возвращать $f(x) + \delta (x)$. Более того, шум может быть стохастическим \cite{lucchi2022theoretical} или детерминированным \cite{lobanov2023zeroorder}.\\
Другую важную роль в решении оптимизационных задач играют ускоренные методы. Они как правило имеют более быструю сходимость по сравнению со стандартными алгоритмами. Предложенный Нестеровым \cite{Nesterov1983AMF} быстрый градиентный метод является классическим примером.
\subsection{Смежные работы}
Впервые метод JAGUAR аппроксимации градиента был предложен в \cite{bogdanov2024aspects}, доказана сходимость для методов Франка-Вульфа и градиентного спуска для невыпуклой, выпуклой, и сильно-выпуклой задач. Также есть модификация алгоритма Франка-Вульфа для стохастического случая (рассмотрены случаи one-point feedback и two-point feedback).\\
В работе \cite{beznosikov2023order} рассмотрены ускоренные методы первого порядка в невыпуклых и сильно выпуклых задачах оптимизации, содержащих марковский шум. Там при аппроксимации градиента использовался рандомизированный размер батча. Результаты и подходы оттуда адаптированы под метод JAGUAR в данной работе.\\
State-of-the-art решения в области безградиентной оптимизации собраны в \cite{gasnikov2024randomized}. Представлены рандомизированные методы нулевого порядка, однако лишь для нестохастического случая.\\
Ускоренный безградиентный метод в гладкой выпуклой стохастической оптимизации был предложен в \cite{gorbunov2020accelerated}, но лишь в случае two-point feedback, который далеко не всегда реализуется на практике.
\subsection{Предложения}
Предлагается ускоренный метод, в качестве аппроксимации градиента использующий JAGUAR. Для него получены теоремы о скорости сходимости при разных ограничениях на целевую функцию (выпуклая, сильно-выпуклая, невыпуклая). Теоретическая часть подкреплена численным экспериментом: классификация на датасете небольшого размера и минимизация квадратичной функции. Проведено сравнение с методами $l_2$-smoothing и полной аппроксимацией градиента.

\section{Постановка задачи}
\bibliographystyle{plain}
\bibliography{Khafizov2024AcceleratedZeroOrderMethods}

\end{document}
